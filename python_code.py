# -*- coding: utf-8 -*-
"""notebook_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PxIaJyExZS1FCGKtSa1JTVBBla3vvU52

# Proyek Analisis Data: **Memprediksi risiko kredit (credit risk)**

## Import Semua Packages/Library yang Digunakan
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, OrdinalEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.feature_selection import SelectKBest, f_classif
from xgboost import XGBClassifier

plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

"""## Data Wrangling

### Gathering Data
"""

from google.colab import drive
drive.mount('/content/drive')

files_dir = os.listdir("/content/drive/MyDrive/Rakamin Internship/Final Case")
print(files_dir)

pd.set_option('display.max_columns', 75)
data = pd.read_csv("/content/drive/MyDrive/Rakamin Internship/Final Case/loan_data_2007_2014.csv")
data.head()

"""### Assessing Data"""

print("============ Total Ukuran ============ ")
data.shape

data.info()

list_null_percentage = []

for col in data.columns:
  if data[col].isnull().sum() != 0:
    null_percentage = ((data[col].isnull().sum()) / len(data)) * 100
    list_null_percentage.append([col, null_percentage])

list_null_percentage.sort(key=lambda x: x[1], reverse=True)

print("============ Data Null Percent from 466285 row data ============ \n")
for i in list_null_percentage:
  print(f'{i[0]} : {i[1]:.3f}%')

def look_object_in_data(data):
  list_data_object = data.select_dtypes(include=['object']).columns.tolist()

  for obj in list_data_object:
      print(f'{obj} : {data[obj].unique()}')

look_object_in_data(data)

"""### Cleaning Data

#### Encode Risk Label
"""

risk_mapping = {
        'Fully Paid': 0,  # Good
        'Current': 0,     # Good
        'Charged Off': 1, # Bad
        'Default': 1,     # Bad
        'Late (31-120 days)': 1,  # Bad
        'Late (16-30 days)': 1,   # Bad
        'In Grace Period': 1,     # Bad
        'Does not meet the credit policy. Status:Fully Paid': 0,  # Good
        'Does not meet the credit policy. Status:Charged Off': 1  # Bad
    }

data['risk_label'] = data['loan_status'].map(risk_mapping)

# Handle any unmapped values
if data['risk_label'].isnull().any():
    print("Warning: Some loan statuses couldn't be mapped!")
    print(data[data['risk_label'].isnull()]['loan_status'].value_counts())

"""#### Drop Kolom"""

columns_to_drop = [
    'Unnamed: 0',
    'id',
    'member_id',
    'emp_title',
    'issue_d',
    'url',
    'desc',
    'title',
    'annual_inc_joint',
    'dti_joint',
    'verification_status_joint',
    'open_acc_6m',
    'open_il_6m',
    'open_il_12m',
    'open_il_24m',
    'mths_since_rcnt_il',
    'total_bal_il',
    'il_util',
    'open_rv_12m',
    'open_rv_24m',
    'max_bal_bc',
    'all_util',
    'inq_fi',
    'total_cu_tl',
    'inq_last_12m',
    'mths_since_last_record',
    'mths_since_last_major_derog',
    'mths_since_last_delinq',
    'next_pymnt_d',
    'last_pymnt_d',
    'last_credit_pull_d',
    'earliest_cr_line',
    'initial_list_status',
    'last_credit_pull_d',
    'application_type',
    'zip_code',
    'pymnt_plan',
    'policy_code'
]

data.drop(columns=columns_to_drop, inplace=True)

"""#### Fill kolom

"""

# 15% missing
impute_cols_median = ['tot_coll_amt',
                      'tot_cur_bal',
                      'total_rev_hi_lim']

for col in impute_cols_median:
    data[col] = data[col].fillna(data[col].median())

# 5-6% missing
impute_cols_mode = ['emp_length',
                    'revol_util']

for col in impute_cols_mode:
    data[col] = data[col].fillna(data[col].mode()[0])

# < 1% missing
impute_cols_misc = ['collections_12_mths_ex_med',
                    'delinq_2yrs',
                    'inq_last_6mths',
                    'open_acc',
                    'pub_rec',
                    'total_acc',
                    'acc_now_delinq',
                    'annual_inc']

for col in impute_cols_misc:
    if data[col].dtype == 'object':
        data[col] = data[col].fillna(data[col].mode()[0])
    else:
        data[col] = data[col].fillna(data[col].median())

data.dropna(inplace=True)

data.drop_duplicates(inplace=True)

"""### Pengecekan Kembali"""

data.shape

pd.set_option('display.max_columns', 42)
data.sample(10)

data.describe().T

list_null_percentage = []

for col in data.columns:
  if data[col].isnull().sum() != 0:
    null_percentage = ((data[col].isnull().sum()) / len(data)) * 100
    list_null_percentage.append([col, null_percentage])

list_null_percentage.sort(key=lambda x: x[1], reverse=True)

print("============ Data Null Percent from 466285 row data ============ \n")
for i in list_null_percentage:
  print(f'{i[0]} : {i[1]:.3f}%')

look_object_in_data(data)

"""## Exploratory Data Analysis (EDA)"""

plt.figure(figsize=(22, 18))
corr = data.select_dtypes(include=['float64', 'int64']).corr()

print("Correlation Matrix (rounded to 2 decimals):\n")
print(corr.round(2).to_string())

plt.figure(figsize=(22, 18))
sns.heatmap(data=corr, annot=True, cmap='coolwarm')

print("\nBASIC DATASET INFORMATION")
print("-" * 30)
print(f"Total Records: {len(data):,}")
print(f"Total Features: {data.shape[1]}")
print(f"Missing Values: {data.isnull().sum().sum():,}")
print(f"Duplicate Records: {data.duplicated().sum():,}")

print(f"\nData Types:")
print(data.dtypes.value_counts())

print("\nMISSING VALUES ANALYSIS")
print("-" * 30)
missing_data = data.isnull().sum()
missing_data = missing_data[missing_data > 0].sort_values(ascending=False)
if len(missing_data) > 0:
    missing_pct = (missing_data / len(data)) * 100
    missing_df = pd.DataFrame({
        'Missing Count': missing_data,
        'Percentage': missing_pct
    })
    print(missing_df.head(10))
else:
    print("No missing values found!")

print("\nNUMERICAL FEATURES ANALYSIS")
print("-" * 30)
numerical_cols = data.select_dtypes(include=[np.number]).columns.tolist()
print(f"Numerical Features: {len(numerical_cols)}")

key_numerical = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'revol_util']
for col in key_numerical:
    if col in data.columns:
        print(f"\n{col.upper()}:")
        print(f"  Mean: {data[col].mean():.2f}")
        print(f"  Median: {data[col].median():.2f}")
        print(f"  Std: {data[col].std():.2f}")
        print(f"  Min-Max: {data[col].min():.2f} - {data[col].max():.2f}")

print("\nCATEGORICAL FEATURES ANALYSIS")
print("-" * 30)
categorical_cols = data.select_dtypes(include=['object']).columns.tolist()
print(f"Categorical Features: {len(categorical_cols)}")

for col in ['grade', 'purpose', 'home_ownership', 'verification_status']:
    if col in data.columns:
        print(f"\n{col.upper()}:")
        value_counts = data[col].value_counts().head(5)
        for idx, count in value_counts.items():
            pct = (count / len(data)) * 100
            print(f"  {idx}: {count:,} ({pct:.1f}%)")

"""## Visualization & Explanatory Analysis"""

plt.figure(figsize=(8, 5))
sns.histplot(data['loan_amnt'], bins=50, color='skyblue', edgecolor='black', kde=False)

plt.xlabel('Jumlah Pinjaman (Loan Amount) ($)')
plt.ylabel('Frekuensi')
plt.title('Distribusi Jumlah Pinjaman', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 5))
sns.histplot(data['int_rate'], bins=50, color='lightcoral', edgecolor='black', kde=False)

plt.xlabel('Tingkat Bunga (Interest Rate) (%)')
plt.ylabel('Frekuensi')
plt.title('Distribusi Tingkat Bunga', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# Hitung jumlah grade dan urutkan
grade_counts = data['grade'].value_counts().sort_index()
grade_df = grade_counts.reset_index()
grade_df.columns = ['Grade', 'Count']

# Plot dengan seaborn
plt.figure(figsize=(8, 5))
sns.barplot(data=grade_df, x='Grade', y='Count', color='lightgreen', edgecolor='black')

plt.xlabel('Grade')
plt.ylabel('Jumlah')
plt.title('Distribusi Grade Kredit', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# Aggregate loan amount by grade and purpose
df_grouped = data.groupby(['grade', 'purpose'])['loan_amnt'].sum().reset_index()

# Create bar plot
plt.figure(figsize=(18, 10))
sns.barplot(data=df_grouped, x='grade', y='loan_amnt', hue='purpose', palette='pastel')
plt.xlabel('Grade')
plt.ylabel('Total Loan Amount ($)')
plt.title('Total Loan Amount by Grade and Purpose')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Purpose', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Hitung dan urutkan jumlah tujuan pinjaman
purpose_counts = data['purpose'].value_counts(ascending=False)
purpose_df = purpose_counts.reset_index()
purpose_df.columns = ['Purpose', 'Count']

# Plot horizontal bar chart dengan seaborn
plt.figure(figsize=(8, 6))
sns.barplot(data=purpose_df, x='Count', y='Purpose', color='orange', edgecolor='black')

plt.xlabel('Jumlah')
plt.ylabel('Tujuan Pinjaman')
plt.title('Tujuan Pinjaman yang Paling Umum', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# Hitung rata-rata label risiko (asumsinya: 1 = default/buruk, 0 = baik)
risk_by_grade = data.groupby('grade')['risk_label'].mean().sort_index()
risk_df = risk_by_grade.reset_index()
risk_df.columns = ['Grade', 'Default_Rate']

# Plot dengan seaborn
plt.figure(figsize=(8, 5))
sns.barplot(data=risk_df, x='Grade', y='Default_Rate', color='red', edgecolor='black')

plt.xlabel('Grade')
plt.ylabel('Tingkat Gagal Bayar (Default Rate)')
plt.title('Tingkat Gagal Bayar per Grade', fontsize=14, fontweight='bold')
plt.ylim(0, 1)  # Default rate antara 0â€“1
plt.tight_layout()
plt.show()

# Plot boxplot dengan seaborn
plt.figure(figsize=(8, 5))
sns.boxplot(data=data, x='risk_label', y='annual_inc', palette='pastel')

plt.yscale('log')  # Skala logaritmik untuk mengatasi outlier
plt.xlabel('Risiko Kredit')
plt.ylabel('Pendapatan Tahunan ($)')
plt.title('Pendapatan Tahunan Berdasarkan Risiko Kredit', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# Tambahkan label teks untuk risiko (jika belum)
data['risk_text'] = data['risk_label'].map({0: 'Good', 1: 'Bad'})

# Plot histogram menggunakan seaborn
plt.figure(figsize=(8, 5))
sns.histplot(data=data, x='dti', hue='risk_text', bins=30, element='step', stat='count', palette={'Good': 'green', 'Bad': 'red'}, alpha=0.5)

plt.xlabel('Rasio Utang terhadap Pendapatan (DTI)')
plt.ylabel('Frekuensi')
plt.title('Distribusi DTI Berdasarkan Risiko Kredit', fontsize=14, fontweight='bold')
plt.legend(title='Label Risiko')
plt.tight_layout()
plt.show()

# Hitung rata-rata risiko berdasarkan panjang kerja
emp_risk = data.groupby('emp_length')['risk_label'].mean().reset_index()
emp_risk.columns = ['Employment Length', 'Default Rate']

# Plot dengan seaborn
plt.figure(figsize=(10, 5))
sns.barplot(data=emp_risk, x='Employment Length', y='Default Rate', color='purple', edgecolor='black')

plt.xticks(rotation=45)
plt.ylabel('Tingkat Gagal Bayar (Default Rate)')
plt.xlabel('Lama Bekerja')
plt.title('Tingkat Gagal Bayar Berdasarkan Lama Bekerja', fontsize=14, fontweight='bold')
plt.ylim(0, 0.14)
plt.tight_layout()
plt.show()

# Plot scatterplot dengan seaborn
plt.figure(figsize=(8, 5))
sns.scatterplot(
    data=data,
    x='loan_amnt',
    y='int_rate',
    hue='risk_label',
    palette={0: 'green', 1: 'red'},
    alpha=0.1,
    s=10
)

plt.xlabel('Jumlah Pinjaman (Loan Amount) ($)')
plt.ylabel('Tingkat Bunga (Interest Rate) (%)')
plt.title('Jumlah Pinjaman vs Tingkat Bunga Berdasarkan Risiko', fontsize=14, fontweight='bold')
plt.legend(title='Risiko Kredit')
plt.tight_layout()
plt.show()

# Hitung rata-rata risiko berdasarkan status verifikasi
verif_risk = data.groupby('verification_status')['risk_label'].mean().reset_index()
verif_risk.columns = ['Verification Status', 'Default Rate']

# Plot dengan seaborn
plt.figure(figsize=(8, 5))
ax = sns.barplot(data=verif_risk, x='Verification Status', y='Default Rate', color='teal', edgecolor='black')

plt.xlabel('Status Verifikasi')
plt.ylabel('Tingkat Gagal Bayar (Default Rate)')
plt.title('Tingkat Gagal Bayar Berdasarkan Status Verifikasi', fontsize=14, fontweight='bold')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""## Analisis Lanjutan (Data Modelling)"""

data_modelling = data.copy()

risk_counts = data_modelling['risk_label'].value_counts()
plt.title('Risk Distribution', fontsize=14, fontweight='bold')
plt.pie(x=risk_counts.values, labels=['Good', 'Bad'], autopct='%1.1f%%', startangle=90)

sns.countplot(data=data_modelling, x='risk_label')

look_object_in_data(data_modelling)

# TERM (36 = 0, 60 = 1)
data_modelling['term'] = data_modelling['term'].map({' 36 months': 0, ' 60 months': 1})

# GRADE
grade_order = ['A', 'B', 'C', 'D', 'E', 'F', 'G']
grade_encoder = OrdinalEncoder(categories=[grade_order])
data_modelling['grade'] = grade_encoder.fit_transform(data_modelling[['grade']]).astype(int)

# EMP_LENGTH
emp_length_mapping = {
    '< 1 year': 0, '1 year': 1, '2 years': 2, '3 years': 3,
    '4 years': 4, '5 years': 5, '6 years': 6, '7 years': 7,
    '8 years': 8, '9 years': 9, '10+ years': 10
}

data_modelling['emp_length'] = data_modelling['emp_length'].map(emp_length_mapping)

# HOME_OWNERSHIP
home_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False) # Added handle_unknown and sparse_output
home_ownership_encoded = home_encoder.fit_transform(data_modelling[['home_ownership']]) # Reshape the input

home_ownership_df = pd.DataFrame(home_ownership_encoded, columns=home_encoder.get_feature_names_out(['home_ownership']), index=data_modelling.index)
data_modelling = pd.concat([data_modelling.drop('home_ownership', axis=1), home_ownership_df], axis=1)


# VERIFICATION_STATUS
verif_encoder = LabelEncoder()
data_modelling['verification_status'] = verif_encoder.fit_transform(data_modelling['verification_status'])

# PURPOSE
purpose_encoder = LabelEncoder()
data_modelling['purpose'] = purpose_encoder.fit_transform(data_modelling['purpose'])

# Separate features and target
X = data_modelling.drop(['risk_label', 'loan_status', 'sub_grade', 'addr_state', 'risk_text'], axis=1, errors='ignore')
y = data_modelling['risk_label']

# Select only numeric columns for modeling
numeric_cols = X.select_dtypes(include=[np.number]).columns
X = X[numeric_cols]

# Handle any remaining missing values
X = X.fillna(X.mean())

# Remove highly correlated features
corr_matrix = X.corr().abs()
upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]
X = X.drop(to_drop, axis=1)

print(f"Features after correlation removal: {X.shape[1]}")
print(f"Features removed due to high correlation: {len(to_drop)}")

print(f"\nFinal feature matrix shape: {X.shape}")
print(f"Target variable shape: {y.shape}")
print(f"Class distribution: {y.value_counts().to_dict()}")

from imblearn.combine import SMOTEENN

smoteenn = SMOTEENN(random_state=42)
X_resampled, y_resampled = smoteenn.fit_resample(X, y)

X_train, X_test, y_train, y_test = train_test_split(X_resampled,
                                                    y_resampled,
                                                    test_size=0.2,
                                                    random_state=42,
                                                    stratify=y_resampled)

print(f"\nTraining set: {X_train.shape}")
print(f"Test set: {X_test.shape}")

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("\nFeatures scaled successfully!")

models = {
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),
    'Neural Network': MLPClassifier(random_state=42, max_iter=500)
}

model_results = {}

for name, model in models.items():
    print(f"\n{'='*20} {name} {'='*20}")
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba)

    model_results[name] = {
        'model': model,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'roc_auc': roc_auc,
        'y_pred': y_pred,
        'y_pred_proba': y_pred_proba
    }

    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")
    print(f"ROC-AUC: {roc_auc:.4f}")

comparison_df = pd.DataFrame({
    'Model': list(model_results.keys()),
    'Accuracy': [r['accuracy'] for r in model_results.values()],
    'Precision': [r['precision'] for r in model_results.values()],
    'Recall': [r['recall'] for r in model_results.values()],
    'F1-Score': [r['f1'] for r in model_results.values()],
    'ROC-AUC': [r['roc_auc'] for r in model_results.values()]
})

print("\nModel Performance Comparison:")
print(comparison_df.round(4))

best_model_name = comparison_df.loc[comparison_df['ROC-AUC'].idxmax(), 'Model']
best_model = model_results[best_model_name]['model']

print(f"\nBest Model: {best_model_name}")
print(f"Best ROC-AUC Score: {comparison_df['ROC-AUC'].max():.4f}")

if best_model_name == 'Random Forest':
    param_grid = {
        'n_estimators': [100, 200],
        'max_depth': [10, 20, None],
        'min_samples_split': [2, 5],
        'min_samples_leaf': [1, 2]
    }
    model = RandomForestClassifier(random_state=42)

elif best_model_name == 'XGBoost':
    param_grid = {
        'n_estimators': [100, 200],
        'max_depth': [3, 6, 10],
        'learning_rate': [0.01, 0.1, 0.2],
        'subsample': [0.8, 1.0]
    }
    model = XGBClassifier(random_state=42, eval_metric='logloss')

elif best_model_name == 'Gradient Boosting':
    param_grid = {
        'n_estimators': [100, 200],
        'max_depth': [3, 5, 7],
        'learning_rate': [0.01, 0.1, 0.2]
    }
    model = GradientBoostingClassifier(random_state=42)

else:
    print(f"No tuning parameters defined for {best_model_name}")
    tuned_model = None

if best_model_name in ['Random Forest', 'XGBoost', 'Gradient Boosting']:
    print(f"Tuning {best_model_name}...")
    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='roc_auc', n_jobs=-1)
    grid_search.fit(X_train_scaled, y_train)
    tuned_model = grid_search.best_estimator_

    print(f"Best parameters: {grid_search.best_params_}")
    print(f"Best cross-validation score: {grid_search.best_score_:.4f}")

    y_pred_tuned = tuned_model.predict(X_test_scaled)
    y_pred_proba_tuned = tuned_model.predict_proba(X_test_scaled)[:, 1]

    tuned_accuracy = accuracy_score(y_test, y_pred_tuned)
    tuned_precision = precision_score(y_test, y_pred_tuned)
    tuned_recall = recall_score(y_test, y_pred_tuned)
    tuned_f1 = f1_score(y_test, y_pred_tuned)
    tuned_roc_auc = roc_auc_score(y_test, y_pred_proba_tuned)

    print(f"\nTuned {best_model_name} Performance:")
    print(f"Accuracy: {tuned_accuracy:.4f}")
    print(f"Precision: {tuned_precision:.4f}")
    print(f"Recall: {tuned_recall:.4f}")
    print(f"F1-Score: {tuned_f1:.4f}")
    print(f"ROC-AUC: {tuned_roc_auc:.4f}")

final_model = tuned_model if 'tuned_model' in locals() and tuned_model else best_model

if hasattr(final_model, 'feature_importances_'):
    importances = final_model.feature_importances_
    feature_importance_df = pd.DataFrame({
        'feature': X.columns,
        'importance': importances
    }).sort_values('importance', ascending=False)

    print("Top Feature Importances:")
    print(feature_importance_df.head(15))

    plt.figure(figsize=(12, 8))
    top_features = feature_importance_df.head(15)
    plt.barh(top_features['feature'], top_features['importance'])
    plt.xlabel('Feature Importance')
    plt.title('Top 15 Feature Importances')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()

y_pred_final = final_model.predict(X_test_scaled)
y_pred_proba_final = final_model.predict_proba(X_test_scaled)[:, 1]

final_accuracy = accuracy_score(y_test, y_pred_final)
final_precision = precision_score(y_test, y_pred_final)
final_recall = recall_score(y_test, y_pred_final)
final_f1 = f1_score(y_test, y_pred_final)
final_roc_auc = roc_auc_score(y_test, y_pred_proba_final)

print(f"Accuracy: {final_accuracy:.4f}")
print(f"Precision: {final_precision:.4f}")
print(f"Recall: {final_recall:.4f}")
print(f"F1-Score: {final_f1:.4f}")
print(f"ROC-AUC: {final_roc_auc:.4f}")